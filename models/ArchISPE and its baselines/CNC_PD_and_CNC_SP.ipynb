{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHamuFXaSVF0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import statistical_analysis as sa\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import data_helpers\n",
        "from text_cnn import TextCNN\n",
        "from tensorflow.contrib import learn\n",
        "import numpy as np\n",
        "from sys import stdin\n",
        "import csv\n",
        "\n",
        "\n",
        "class TextCNN(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "      self, sequence_length, num_classes, vocab_size,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "        self.phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                conv = tf.nn.bias_add(conv, b)\n",
        "\n",
        "                #batch normalization\n",
        "                beta = tf.Variable(tf.constant(0.0, shape=[num_filters]),\n",
        "                                   name='beta', trainable=True)\n",
        "                gamma = tf.Variable(tf.constant(1.0, shape=[num_filters]),\n",
        "                                    name='gamma', trainable=True)\n",
        "                batch_mean, batch_var = tf.nn.moments(conv, [0, 1, 2], name='moments')\n",
        "                ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "                def mean_var_with_update():\n",
        "                    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "                    with tf.control_dependencies([ema_apply_op]):\n",
        "                        return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "                mean, var = tf.cond(self.phase_train,\n",
        "                                    mean_var_with_update,\n",
        "                                    lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "                conv_bn = tf.nn.batch_normalization(conv, mean, var, beta, gamma, 1e-3)\n",
        "\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(conv_bn, name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(3, pooled_outputs) #for tf_0.12\n",
        "        #self.h_pool = tf.concat(pooled_outputs, 3) #for tf 1.2.1\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            self.l2_loss += tf.nn.l2_loss(W)\n",
        "            self.l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.softmax(tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\"))\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y) #for tf_0.12\n",
        "            #losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y) #for tf 1.2.1\n",
        "            self.mean_loss = tf.reduce_mean(losses)\n",
        "            self.loss = self.mean_loss + l2_reg_lambda * self.l2_loss\n",
        "\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "\n",
        "\n",
        "#############################################play.py########################################################################################\n",
        "\n",
        "# Parameters\n",
        "\n",
        "# Data loading params\n",
        "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
        "tf.flags.DEFINE_string(\"source\", \"tensorflow docker vscode bootstrap\", \"source file, split by space\")\n",
        "tf.flags.DEFINE_string(\"target\", \"DECA\", \"target file\")\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 192, \"Dimensionality of character embedding (default: 128)\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"2,3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.2, \"L2 regularizaion lambda (default: 0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 1000)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 1, \"Evaluate model on dev set after this many epochs (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 5000, \"Save model after this many steps (default: 100)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS._parse_flags()\n",
        "\n",
        "\n",
        "#sentence class\n",
        "class_names = ( 'solution proposal', 'problem discovery')\n",
        "\n",
        "\n",
        "def train_model(allow_save_model = False, print_intermediate_results = True):\n",
        "\n",
        "    print(\"\\nParameters:\")\n",
        "    for attr, value in sorted(FLAGS.__flags.items()):\n",
        "        print(\"{}={}\".format(attr.upper(), value))\n",
        "    print(\"\")\n",
        "\n",
        "    # Data Preparatopn\n",
        "    # ==================================================\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    #load each source file\n",
        "    dataset = FLAGS.source.split()\n",
        "    source_text = np.array([])\n",
        "    source_y = np.array([])\n",
        "    for post in dataset:\n",
        "        source_file_path = \"data/\" + post + \"/\"\n",
        "        source_files = list()\n",
        "        for class_name in class_names:\n",
        "            source_files.append(source_file_path + class_name)\n",
        "        tmp_text, tmp_y = data_helpers.load_data_and_labels(source_files)\n",
        "        print post +\": \"+str(len(tmp_text))+\" sentences\"\n",
        "        source_text = np.concatenate([source_text, tmp_text], 0)\n",
        "        if len(source_y)==0:\n",
        "            source_y = np.array(tmp_y)\n",
        "        else:\n",
        "            source_y = np.concatenate([source_y, tmp_y], 0)\n",
        "\n",
        "\n",
        "    posts = FLAGS.target.split()\n",
        "    target_text = np.array([])\n",
        "    target_y = np.array([])\n",
        "    for post in dataset:\n",
        "        target_file_path = \"data/\" + post + \"/\"\n",
        "        target_files = list()\n",
        "        for class_name in class_names:\n",
        "            target_files.append(target_file_path + class_name)\n",
        "        tmp_text, tmp_y = data_helpers.load_data_and_labels(target_files)\n",
        "        print post + \": \" + str(len(tmp_text)) + \" sentences\"\n",
        "        target_text = np.concatenate([target_text, tmp_text], 0)\n",
        "        if len(target_y) == 0:\n",
        "            target_y = np.array(tmp_y)\n",
        "        else:\n",
        "            target_y = np.concatenate([target_y, tmp_y], 0)\n",
        "\n",
        "\n",
        "    all_text = np.concatenate([source_text, target_text], 0)\n",
        "\n",
        "    # Build vocabulary\n",
        "    max_document_length = max([len(x.split(\" \")) for x in all_text])\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    source_x = np.array(list(vocab_processor.fit_transform(source_text)))\n",
        "    target_x = np.array(list(vocab_processor.fit_transform(target_text)))\n",
        "\n",
        "    if print_intermediate_results:\n",
        "        print('data distribution in source dataset')\n",
        "        sa.print_data_distribution(source_y, class_names)\n",
        "        print('data distribution in target dataset')\n",
        "        sa.print_data_distribution(target_y, class_names)\n",
        "\n",
        "        print(\"Max Document Length: {:d}\".format(max_document_length))\n",
        "        print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "        print(\"Train/Test size: {:d}/{:d}\".format(len(source_y), len(target_y)))\n",
        "\n",
        "    # Training\n",
        "    # ==================================================\n",
        "\n",
        "    min_loss = 100000000\n",
        "    predictions_at_min_loss = None\n",
        "    steps_per_epoch = (int)(len(source_y) / FLAGS.batch_size) + 1\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "            log_device_placement=FLAGS.log_device_placement)\n",
        "        session_conf.gpu_options.allow_growth = True\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            cnn = TextCNN(\n",
        "                sequence_length=source_x.shape[1],\n",
        "                num_classes=source_y.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=FLAGS.embedding_dim,\n",
        "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "                num_filters=FLAGS.num_filters,\n",
        "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "            # Define Training procedure\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "            learning_rate = tf.train.polynomial_decay(2*1e-3, global_step,\n",
        "                                                      steps_per_epoch * FLAGS.num_epochs, 1e-4,\n",
        "                                                      power=1)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "            if allow_save_model:\n",
        "\n",
        "                # Output directory for models and summaries\n",
        "                timestamp = str(int(time.time()))\n",
        "                out_dir = os.path.abspath(\n",
        "                    os.path.join(os.path.curdir, \"runs\", FLAGS.source))\n",
        "                print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "                # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "                checkpoint_dir_name = \"checkpoint-\" + str(FLAGS.embedding_dim) + \"-\" + FLAGS.filter_sizes + \"-\" + \\\n",
        "                                      str(FLAGS.num_filters) + \"-\" + str(FLAGS.dropout_keep_prob) + \"-\" + str(\n",
        "                    FLAGS.l2_reg_lambda) + \\\n",
        "                                      \"-\" + str(FLAGS.batch_size) + \"-\" + str(FLAGS.num_epochs)\n",
        "                checkpoint_dir = os.path.abspath(os.path.join(out_dir, checkpoint_dir_name))\n",
        "                checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "                if not os.path.exists(checkpoint_dir):\n",
        "                    os.makedirs(checkpoint_dir)\n",
        "                saver = tf.train.Saver(tf.all_variables())\n",
        "\n",
        "                # Write vocabulary\n",
        "                vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "            # Initialize all variables\n",
        "            sess.run(tf.global_variables_initializer(), feed_dict={cnn.phase_train: True})  # this is for version r0.12\n",
        "\n",
        "            def train_step(x_batch, y_batch):\n",
        "                \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                    cnn.input_x: x_batch,\n",
        "                    cnn.input_y: y_batch,\n",
        "                    cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
        "                    cnn.phase_train: True\n",
        "                }\n",
        "                _, step, loss, mean_loss, l2_loss, accuracy = sess.run(\n",
        "                    [train_op, global_step, cnn.loss, cnn.mean_loss, cnn.l2_loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                # print(\"{}: step {}, loss {:g}, acc {:g}, mean_loss {}, l2_loss {}\".format(time_str, step, loss, accuracy, mean_loss, l2_loss))\n",
        "                return accuracy\n",
        "\n",
        "            def dev_step(x_batch, y_batch, writer=None):\n",
        "                \"\"\"\n",
        "                Evaluates model on a dev set\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                    cnn.input_x: x_batch,\n",
        "                    cnn.input_y: y_batch,\n",
        "                    cnn.dropout_keep_prob: 1.0,\n",
        "                    cnn.phase_train: False\n",
        "                }\n",
        "                step, loss, mean_loss, l2_loss, accuracy, predictions = sess.run(\n",
        "                    [global_step, cnn.loss, cnn.mean_loss, cnn.l2_loss, cnn.accuracy, cnn.predictions],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "\n",
        "                if print_intermediate_results:\n",
        "                    print(\"{}: epoch {}, step {}, loss {:g}, acc {:g}, mean_loss {}, l2_loss {}\".format(\n",
        "                        time_str, step/steps_per_epoch, step, loss, accuracy, mean_loss, l2_loss))\n",
        "                return accuracy, loss, predictions\n",
        "\n",
        "            # Generate batches\n",
        "            batches = data_helpers.batch_iter(\n",
        "                list(zip(source_x, source_y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "            # Training loop. For each batch...\n",
        "\n",
        "            for batch in batches:\n",
        "                x_batch, y_batch = zip(*batch)\n",
        "                train_accuracy = train_step(x_batch, y_batch)\n",
        "                current_step = tf.train.global_step(sess, global_step)\n",
        "                current_epoch = current_step/steps_per_epoch\n",
        "                if current_step%steps_per_epoch==0 and current_epoch % FLAGS.evaluate_every == 0:\n",
        "                    if print_intermediate_results:\n",
        "                        print(\"Current train accuracy: %s\\nEvaluation:\" % (train_accuracy))\n",
        "\n",
        "                    fold_accuracy, loss, predictions = dev_step(target_x, target_y)\n",
        "                    #ensemble_prediction([predictions], target_y)\n",
        "                    if loss < min_loss:\n",
        "                        min_loss = loss\n",
        "                        predictions_at_min_loss = predictions\n",
        "                        if allow_save_model:\n",
        "                            save_path = saver.save(sess, checkpoint_prefix)\n",
        "                            if print_intermediate_results:\n",
        "                                print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "\n",
        "            # Final result\n",
        "            output_file = open(target_file_path + 'fp_sentences', 'w')\n",
        "            print('Final result:')\n",
        "            fold_accuracy, loss, predictions = dev_step(target_x, target_y)\n",
        "            print(\"ACC: %s\" % (fold_accuracy))\n",
        "            tp, fp, fn, precision, recall, f1 = sa.calculate_IR_metrics(target_text, target_y, predictions, class_names,\n",
        "                                                                        output_file)\n",
        "            for i in range(len(class_names)):\n",
        "                print class_names[i], precision[i], recall[i], f1[i]\n",
        "            #print(\"average f1-score: %s\" % (sum(f1) / len(f1)))\n",
        "\n",
        "            output_file.close()\n",
        "\n",
        "    return min_loss, predictions_at_min_loss, target_y\n",
        "\n",
        "def ensemble_prediction (list_predictions, y_classes):\n",
        "\n",
        "    tp, fp, fn, precision, recall, f1 = (np.zeros(len(class_names)) for _ in range(6))\n",
        "\n",
        "    num_instances = len(y_classes)\n",
        "    votes = list()\n",
        "    for _ in range(num_instances):\n",
        "        votes.append((np.zeros(len(class_names))))\n",
        "\n",
        "\n",
        "    for prediction_epoch in list_predictions:\n",
        "        for instance_index, predicted_class in enumerate(prediction_epoch):\n",
        "            votes[instance_index][predicted_class] += 1\n",
        "\n",
        "    final_prediction = np.zeros(num_instances)\n",
        "\n",
        "    for instance_index,true_class in enumerate(y_classes):\n",
        "        true_class = np.argmax(true_class)\n",
        "        predict_class = np.argmax(votes[instance_index])\n",
        "        final_prediction[instance_index] = predict_class\n",
        "        if predict_class == true_class:\n",
        "            tp[true_class] += 1\n",
        "        if predict_class != true_class:\n",
        "            fn[true_class] += 1\n",
        "            fp[predict_class] += 1\n",
        "\n",
        "    #print (\"final accuracy:\",sum(tp)*1.0/len(y_classes))\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        precision[i] = tp[i] / (tp[i] + fp[i])\n",
        "        recall[i] = tp[i] / (tp[i] + fn[i])\n",
        "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        print class_names[i], 'precision: ', precision[i], ', recall: ', recall[i], ', f1-score: ', f1[i]\n",
        "\n",
        "\n",
        "def auto_param_tuning():\n",
        "\n",
        "\n",
        "    tmp = FLAGS.embedding_dim\n",
        "    best_embedding_dim = FLAGS.embedding_dim\n",
        "    min_loss = 100000000\n",
        "    list_embedding_dim = {128, 192, 256, 320}\n",
        "    for embedding_dim in list_embedding_dim:\n",
        "        FLAGS.embedding_dim = embedding_dim\n",
        "        loss, predictions, target_y = train_model(False,False)\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            best_embedding_dim = embedding_dim\n",
        "    FLAGS.embedding_dim = tmp\n",
        "\n",
        "\n",
        "    tmp = FLAGS.num_filters\n",
        "    best_num_filters = FLAGS.num_filters\n",
        "    min_loss = 100000000\n",
        "    list_num_filters = {128, 192, 256, 320}\n",
        "    for num_filters in list_num_filters:\n",
        "        FLAGS.num_filters = num_filters\n",
        "        loss, predictions, target_y = train_model(False,False)\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            best_num_filters = num_filters\n",
        "    FLAGS.num_filters = tmp\n",
        "\n",
        "\n",
        "    FLAGS.embedding_dim = best_embedding_dim\n",
        "    FLAGS.num_filters = best_num_filters\n",
        "\n",
        "    list_predictions = list()\n",
        "    list_filter_sizes = {'1,2,3','2,3,4','3,4,5','4,5,6','2,3,4,5','1,2,3,4','3,4,5,6','1,2,3,4,5','2,3,4,5,6','1,2,3,4,5,6'}\n",
        "    for filter_sizes in list_filter_sizes:\n",
        "        FLAGS.filter_sizes = filter_sizes\n",
        "        loss, predictions, target_y = train_model(False,True)\n",
        "        list_predictions.append(predictions)\n",
        "\n",
        "    ensemble_prediction(list_predictions,target_y)\n",
        "\n",
        "\n",
        "train_model(False,True)\n",
        "\n",
        "#####################################################statistical_analysis.py################################################################################\n",
        "def print_data_distribution (y_classes, class_names):\n",
        "    \"\"\"\n",
        "    :param y_classes: class of each instance, for example, if there are 3 classes, and y[i] is [1,0,0], then instance[i] belongs to class[0]\n",
        "    :param class_names: name of each class\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    count = np.zeros(len(class_names))\n",
        "    for y in y_classes:\n",
        "        class_index = np.argmax(y)\n",
        "        count[class_index] = count[class_index] + 1\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print class_name,count[i]\n",
        "\n",
        "def calculate_IR_metrics (x_text, y_classes, predictions, class_names, print_incorrect_predictions=None):\n",
        "    \"\"\"\n",
        "    :param x_text: original text of each instance\n",
        "    :param y_classes: true class of each instance, notice that y[i] is in form of array[n]\n",
        "    :param predictions: index of predicted class of each instance\n",
        "    :param class_names: name of each class\n",
        "    :return: tp,fp,fn,precision,recall,f1\n",
        "    \"\"\"\n",
        "    tp, fp, fn, precision, recall, f1 = (np.zeros(len(class_names)) for _ in range(6))\n",
        "    for class_index in range(len(class_names)):\n",
        "        for instance_index, predicted_class in enumerate(predictions):\n",
        "\n",
        "            true_class = np.argmax(y_classes[instance_index]) #true class of this instance\n",
        "\n",
        "            if true_class == class_index and predicted_class == class_index:\n",
        "                tp[class_index] = tp[class_index] + 1\n",
        "\n",
        "            if true_class == class_index and predicted_class != class_index:\n",
        "                fn[class_index] = fn[class_index] + 1\n",
        "\n",
        "            if true_class != class_index and predicted_class == class_index:\n",
        "                fp[class_index] = fp[class_index] + 1\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        precision[i] = tp[i] / (tp[i] + fp[i])\n",
        "        recall[i] = tp[i] / (tp[i] + fn[i])\n",
        "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
        "\n",
        "    if print_incorrect_predictions != None:\n",
        "\n",
        "        print 'write incorrect preditions to: ',print_incorrect_predictions\n",
        "\n",
        "        output_file = print_incorrect_predictions\n",
        "        text_to_print = list()\n",
        "\n",
        "        for instance_index, predicted_class in enumerate(predictions):\n",
        "            true_class = np.argmax(y_classes[instance_index])\n",
        "            if predicted_class != true_class:\n",
        "                text_to_print.append(str(instance_index)+' True : '+class_names[true_class]+' Predicted: '+class_names[predicted_class]+' Text: '+x_text[instance_index])\n",
        "\n",
        "        #text_to_print = sorted(text_to_print)\n",
        "        for text in text_to_print:\n",
        "            print >> output_file, text\n",
        "            print (text)\n",
        "\n",
        "        print >> output_file, '-----------------------------------------------------\\n-----------------------------------------------------'\n",
        "\n",
        "\n",
        "    return tp,fp,fn,precision,recall,f1\n",
        "####################################################################################################################################\n",
        "\n",
        "class_names = ( 'solution proposal', 'problem discovery')\n",
        "\n",
        "\n",
        "# Checkpoint Parameters\n",
        "tf.flags.DEFINE_string(\"checkpoint_dir\", \"runs/docker tensorflow bootstrap vscode/checkpoint-256-2,3,4,5-192-0.5-0.2-64-30\", \"Checkpoint directory from training run\")\n",
        "\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS._parse_flags()\n",
        "print(\"\\nParameters:\")\n",
        "for attr, value in sorted(FLAGS.__flags.items()):\n",
        "    print(\"{}={}\".format(attr.upper(), value))\n",
        "print(\"\")\n",
        "\n",
        "# Map data into vocabulary\n",
        "vocab_path = \"runs/docker tensorflow bootstrap vscode/vocab\"\n",
        "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
        "\n",
        "\n",
        "\n",
        "print FLAGS.checkpoint_dir\n",
        "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
        "print (\"current using model: \" + FLAGS.checkpoint_dir)\n",
        "\n",
        "\n",
        "#checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "      log_device_placement=FLAGS.log_device_placement)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        # Load the saved meta graph and restore variables\n",
        "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "        saver.restore(sess, checkpoint_file)\n",
        "\n",
        "        # Get the placeholders from the graph by name\n",
        "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
        "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
        "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
        "        phase_train = graph.get_operation_by_name(\"phase_train\").outputs[0]\n",
        "        conv = graph.get_operation_by_name(\"conv-maxpool-3\").outputs\n",
        "\n",
        "\n",
        "        # Tensors we want to evaluate\n",
        "        predictions_tensor = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
        "\n",
        "        print \"input a sentence and the model will predict a label for you ^__^\"\n",
        "        while True:\n",
        "\n",
        "            text = [stdin.readline()]\n",
        "            x_test = np.array(list(vocab_processor.transform(text)))\n",
        "            predictions = sess.run(predictions_tensor, {input_x: x_test, dropout_keep_prob: 1.0, phase_train: False})\n",
        "            print predictions, class_names[predictions[0]]\n",
        "            print \"\"\n",
        "            print conv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Final result:\n",
        "\n",
        "problem discovery  precision=0.535 recall=0.510 f1=0.420\n",
        "solution proposal  precision=0.472 recall=0.452 f1=0.448\n"
      ],
      "metadata": {
        "id": "kcyUFunovtAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}