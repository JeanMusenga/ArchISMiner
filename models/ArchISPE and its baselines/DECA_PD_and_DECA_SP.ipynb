{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvUVjb8giYqS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from google.colab import files\n",
        "\n",
        "# File Upload\n",
        "print(\"Please upload the file: 366_ARPs_for_extracting_Issue_Solution_Pairs.xlsx\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "input_path = list(uploaded.keys())[0]\n",
        "output_path = \"DECA_PD_SP_results.xlsx\"\n",
        "\n",
        "# Keywords for heuristic rules\n",
        "problem_keywords = [\n",
        "    \"What\", \"When\", \"Who\", \"Which\", \"How\", \"?\", \"I am trying to build\", \"I want to design\",\n",
        "    \"How to architecture\", \"I am evaluating\", \"I am building\", \"The user should\", \"I need help\",\n",
        "    \"I am developing\", \"Advise on\", \"I want to design\", \"crash\", \"error\", \"bug\", \"problem\",\n",
        "    \"issue\", \"wrong\", \"not working\", \"cannot\", \"unable\"\n",
        "]\n",
        "\n",
        "solution_keywords = [\n",
        "    \"the best practice\", \"you should\", \"I am using\", \"you don't have to do\", \"In order to\",\n",
        "    \"it is critical\", \"You should\", \"It is recommended\", \"A good approach is\",\n",
        "    \"I suggest\", \"I propose\", \"fix\", \"I recommend\", \"refactor\", \"to use\", \"to limit\", \"could use\"\n",
        "]\n",
        "\n",
        "# Classification functions\n",
        "def is_problem_discovery(sentence: str) -> bool:\n",
        "    if not isinstance(sentence, str):\n",
        "        return False\n",
        "    sentence_lower = sentence.lower()\n",
        "    return any(kw.lower() in sentence_lower for kw in problem_keywords)\n",
        "\n",
        "def is_solution_proposal(sentence: str) -> bool:\n",
        "    if not isinstance(sentence, str):\n",
        "        return False\n",
        "    sentence_lower = sentence.lower()\n",
        "    return any(kw.lower() in sentence_lower for kw in solution_keywords)\n",
        "\n",
        "def classify_sentence(sentence: str) -> str:\n",
        "    if is_problem_discovery(sentence):\n",
        "        return \"Problem Discovery\"\n",
        "    elif is_solution_proposal(sentence):\n",
        "        return \"Solution Proposal\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Load Excel\n",
        "data = pd.read_excel(input_path, sheet_name=\"Sheet1\")\n",
        "\n",
        "# Apply classification\n",
        "data[\"Question_pred\"] = data[\"Question_body_cleaned\"].apply(classify_sentence)\n",
        "data[\"Answer_pred\"] = data[\"Answer_body_cleaned\"].apply(classify_sentence)\n",
        "\n",
        "#  Evaluation against labeled issues and solutions\n",
        "if \"Reference_Question\" in data.columns and \"Reference_Solution\" in data.columns:\n",
        "    # Evaluation for Problem/Issue (Questions)\n",
        "    q_true = data[\"Reference_Question\"]\n",
        "    q_pred = data[\"Question_pred\"]\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        q_true, q_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    issue_scores = pd.Series({\n",
        "        \"Issue_precision\": precision,\n",
        "        \"Issue_recall\": recall,\n",
        "        \"Issue_f1\": f1\n",
        "    })\n",
        "    print(\"\\nPrecision, Recall, F1 Scores for Problems/Issues:\")\n",
        "    print(issue_scores)\n",
        "\n",
        "    # Evaluation for Solution (Answers)\n",
        "    a_true = data[\"Reference_Solution\"]\n",
        "    a_pred = data[\"Answer_pred\"]\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        a_true, a_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    solution_scores = pd.Series({\n",
        "        \"Solution_precision\": precision,\n",
        "        \"Solution_recall\": recall,\n",
        "        \"Solution_f1\": f1\n",
        "    })\n",
        "    print(\"\\nPrecision, Recall, F1 Scores for Answers/Solutions:\")\n",
        "    print(solution_scores)\n",
        "\n",
        "    # === Save both predictions + evaluation ===\n",
        "    with pd.ExcelWriter(output_path, engine=\"openpyxl\", mode=\"w\") as writer:\n",
        "        data.to_excel(writer, sheet_name=\"Predictions\", index=False)\n",
        "        pd.concat([issue_scores, solution_scores]).to_excel(\n",
        "            writer, sheet_name=\"Evaluation\", header=[\"Value\"]\n",
        "        )\n",
        "\n",
        "    print(f\"\\nSaved predictions and evaluation to {output_path}\")\n",
        "    files.download(output_path)\n",
        "\n",
        "else:\n",
        "    print(\" Labels not found (expected columns: 'Reference_Question', 'Reference_Solution'). Skipping evaluation step.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Precision, Recall, F1 Scores for Problems/Issues:\n",
        "Issue_precision    0.682\n",
        "Issue_recall       0.556\n",
        "Issue_f1           0.540\n",
        "dtype: float64\n",
        "\n",
        "Precision, Recall, F1 Scores for Answers/Solutions:\n",
        "Solution_precision    0.650\n",
        "Solution_recall       0.611\n",
        "Solution_f1           0.571\n",
        "dtype: float64"
      ],
      "metadata": {
        "id": "SYiklDWjnycu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}