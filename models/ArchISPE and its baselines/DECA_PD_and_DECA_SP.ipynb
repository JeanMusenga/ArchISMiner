{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvUVjb8giYqS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "#Config\n",
        "input_path = \"366_ARPs_for_extracting_Issue_Solution_Pairs.xlsx\"\n",
        "output_path = \"DECA_PD_SP_results.xlsx\"\n",
        "\n",
        "# Keywords for heuristic rules\n",
        "problem_keywords = [\n",
        "    \"What\", \"When\", \"Who\", \"Which\", \"How\", \"?\", \"I am trying to build\", \"I want to design\",\n",
        "    \"How to architecture\", \"I am evaluating\", \"I am building\", \"The user should\", \"I need help\",\n",
        "    \"I am developing\", \"Advise on\", \"I want to desing\", \"crash\", \"error\", \"bug\", \"problem\",\n",
        "    \"issue\", \"wrong\", \"not working\", \"cannot\", \"unable\"\n",
        "]\n",
        "\n",
        "solution_keywords = [\n",
        "    \"the best practice\", \"you should\", \"I am using\", \"you don't have to do\", \"In order to\",\n",
        "    \"it is critical\", \"You should\", \"It is recommended\", \"A good approach is\",\n",
        "    \"I suggest\", \"I propose\", \"fix\", \"I recommend\", \"refactor\"\n",
        "]\n",
        "\n",
        "#Classification functions\n",
        "def is_problem_discovery(sentence: str) -> bool:\n",
        "    if not isinstance(sentence, str):\n",
        "        return False\n",
        "    sentence_lower = sentence.lower()\n",
        "    return any(kw.lower() in sentence_lower for kw in problem_keywords)\n",
        "\n",
        "def is_solution_proposal(sentence: str) -> bool:\n",
        "    if not isinstance(sentence, str):\n",
        "        return False\n",
        "    sentence_lower = sentence.lower()\n",
        "    return any(kw.lower() in sentence_lower for kw in solution_keywords)\n",
        "\n",
        "def classify_sentence(sentence: str) -> str:\n",
        "    if is_problem_discovery(sentence):\n",
        "        return \"Problem Discovery\"\n",
        "    elif is_solution_proposal(sentence):\n",
        "        return \"Solution Proposal\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Load Excel\n",
        "data = pd.read_excel(input_path, sheet_name=\"Sheet1\")\n",
        "\n",
        "# Apply classification on question & answer bodies\n",
        "data[\"Question_pred\"] = data[\"Question_body_cleaned\"].apply(classify_sentence)\n",
        "data[\"Answer_pred\"] = data[\"Answer_body_cleaned\"].apply(classify_sentence)\n",
        "\n",
        "# === Evaluation against gold labels ===\n",
        "# Assuming gold labels exist in \"Question_gold\" and \"Answer_gold\"\n",
        "if \"Question_gold\" in data.columns and \"Answer_gold\" in data.columns:\n",
        "    # Combine predictions and gold labels for evaluation\n",
        "    y_true = list(data[\"Question_gold\"]) + list(data[\"Answer_gold\"])\n",
        "    y_pred = list(data[\"Question_pred\"]) + list(data[\"Answer_pred\"])\n",
        "\n",
        "    print(\"\\n=== Evaluation Report ===\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "    # Also export evaluation summary to Excel\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=[\"Problem Discovery\", \"Solution Proposal\", \"Other\"], zero_division=0\n",
        "    )\n",
        "\n",
        "    eval_df = pd.DataFrame({\n",
        "        \"Label\": [\"Problem Discovery\", \"Solution Proposal\", \"Other\"],\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-score\": f1,\n",
        "        \"Support\": support\n",
        "    })\n",
        "\n",
        "    with pd.ExcelWriter(output_path, engine=\"openpyxl\", mode=\"w\") as writer:\n",
        "        data.to_excel(writer, sheet_name=\"Predictions\", index=False)\n",
        "        eval_df.to_excel(writer, sheet_name=\"Evaluation\", index=False)\n",
        "\n",
        "    print(f\"Saved predictions + evaluation results to {output_path}\")\n",
        "else:\n",
        "    print( \"Gold labels not found in the dataset. Skipping evaluation step.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Mean Precision, Recall, F1 Scores for Questions:\n",
        "Question_precision    0.682\n",
        "Question_recall       0.556\n",
        "Question_f1           0.540\n",
        "dtype: float64\n",
        "\n",
        "Mean Precision, Recall, F1 Scores for Answers:\n",
        "Answer_precision    0.650\n",
        "Answer_recall       0.611\n",
        "Answer_f1           0.571\n",
        "dtype: float64"
      ],
      "metadata": {
        "id": "SYiklDWjnycu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}