{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY_W4wFSos9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from scripts.AnswerExtraction.data_helper import *\n",
        "from scripts.AnswerExtraction.evaluation import *\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "from keras.optimizers import adam\n",
        "import re, string\n",
        "import numpy as np, pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scripts.AnswerExtraction import expand_contractions as expand\n",
        "\n",
        "seed(1)\n",
        "set_random_seed(2)\n",
        "\n",
        "#########################################################former_QuestionPattern.py#############################################################\n",
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from spacy.matcher import Matcher\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.attrs import LOWER, POS, ENT_TYPE, LEMMA\n",
        "from spacy.tokens import Doc\n",
        "import numpy\n",
        "\n",
        "rec_target_noun = [\"project\",\"api\",\"tutorial\",\"library\",\"example\",\"book\",\"resource\",\"elm\",\"python\",\"clojure\",\n",
        "                    \"way\",\"practice\",\"approach\",\"strategy\",\"technique\",\"explanation\",\"choice\", \"trick\",\"tip\",\n",
        "                    \"alternative\",\"recommendation\",\"opinion\", \"boilerplate\",\"service\",\"framework\",\"project\",\"package\",\n",
        "                    \"suggestion\",\"linter\",\"procedure\"]\n",
        "action_verbs = [\"have\", \"know\",\"use\",\"try\"]\n",
        "rec_verbs = [\"point\",\"recommend\",\"suggest\",\"advise\",\"tell\"]\n",
        "rec_positive_adjective = [\"good\",\"better\",\"best\",\"right\",\"optimal\",\"ideal\",\"common\",\"established\",\"popular\",\"great\",\n",
        "                          \"nice\",\"nicer\",\"elegant\",\"efficient\",\"easy\",\"easier\",\"simple\",\"simpler\",\"simplistic\",\n",
        "                          \"clean\",\"cleaner\",\"pro\",\"other\",\"idiomatic\",\"pythonic\",\"clojurian\",\"clojure\",\"clojury\",\"elm\",\n",
        "                          \"elmish\",\"generally\",\"accepted\",\"currently\",\"most\",\"normal\",\"ok\",\"appropriate\",\"standard\"]\n",
        "all_caps_noun_regex = \"[A-Z]{2,}s?\"\n",
        "vp_pattern = [{\"POS\": {\"NOT_IN\": [\"VERB\",\"AUX\"]}, \"OP\": \"*\"},\n",
        "              {\"POS\": {\"IN\": [\"VERB\",\"AUX\"]}}]\n",
        "np_pattern = [{\"POS\": {\"NOT_IN\": [\"NOUN\",\"PROPN\"]}, \"OP\": \"*\"},\n",
        "              {\"POS\": {\"IN\": [\"NOUN\",\"PROPN\"]}}]\n",
        "\n",
        "# P_WHAT_RECADJ_TARGETNOUN\n",
        "# Question with postitive adjective and noun\n",
        "# What|Which [VERB_TO_BE] [ADJECTIVE]* <rec_target_noun>\n",
        "def setup_p_what_recadj_targetnoun(matcher):\n",
        "    base_pattern = [{\"LEMMA\": {\"IN\": [\"what\",\"which\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"would\",\"one\"]},\"OP\": \"?\"},\n",
        "                    {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
        "                    {\"POS\": \"DET\",\"OP\": \"?\"},\n",
        "                    {\"POS\": \"ADJ\",\"OP\": \"?\"}]\n",
        "    what_is_pattern1 = base_pattern + [\n",
        "                    {\"LEMMA\": {\"IN\": rec_target_noun}}] + vp_pattern\n",
        "    what_is_pattern2 = base_pattern + [\n",
        "                    {\"TEXT\": {\"REGEX\": all_caps_noun_regex}}] + vp_pattern\n",
        "    matcher.add(\"P_WHAT_RECADJ_TARGETNOUN\",\n",
        "                on_match=on_match_p_what_recadj_targetnoun,\n",
        "                patterns=[what_is_pattern1, what_is_pattern2])\n",
        "\n",
        "def on_match_p_what_recadj_targetnoun(matcher, doc, id, matches):\n",
        "    #print('P_WHAT_RECADJ_TARGETNOUN Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_COULD_ANYONE\n",
        "# Query about the existence of a person with knowledge\n",
        "# Could|Can anyone|someone rec_verb [me|us] [to] [an] <adjective>* <rec_target_noun>\n",
        "def setup_p_could_anyone(matcher):\n",
        "    could_pattern = [{\"LOWER\": {\"IN\": [\"can\", \"could\"]}},\n",
        "                     {\"LOWER\": {\"IN\": [\"someone\", \"anyone\"]}},\n",
        "                     {\"LOWER\": {\"IN\": rec_verbs}}]\n",
        "    matcher.add(\"P_COULD_ANYONE\", on_match_p_could_anyone, could_pattern)\n",
        "\n",
        "def on_match_p_could_anyone(matcher, doc, id, matches):\n",
        "    #print('P_COULD_ANYONE Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_ARE_THERE\n",
        "# Query about the existence an artifact\n",
        "# Is|Are there any|a|an|some <rec_positive_adjective> <rec_target_noun>\n",
        "def setup_p_are_there(matcher):\n",
        "    at_pattern_base = [{\"LOWER\": {\"IN\": [\"are\",\"is\"]}},\n",
        "                     {\"LOWER\": \"there\"},\n",
        "                     {\"LOWER\": {\"IN\": [\"any\", \"a\", \"some\", \"an\"]}},\n",
        "                     {\"LOWER\": {\"IN\": rec_positive_adjective}, \"OP\": \"+\"}]\n",
        "    # at_pattern1 = at_pattern_base + [{\"LEMMA\": {\"IN\": rec_target_noun}}] + vp_pattern\n",
        "    # at_pattern2 = at_pattern_base + [{\"TEXT\": {\"REGEX\": all_caps_noun_regex}}] + vp_pattern\n",
        "    matcher.add(\"P_ARE_THERE\",\n",
        "                on_match = on_match_p_are_there,\n",
        "                patterns = [at_pattern_base])\n",
        "\n",
        "def on_match_p_are_there(matcher, doc, id, matches):\n",
        "    #print('P_ARE_THERE Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_IS_IT\n",
        "def setup_p_is_it(matcher):\n",
        "    is_it_pattern = [{\"LOWER\": \"is\"},\n",
        "                     {\"LOWER\": \"it\"},\n",
        "                     {\"LOWER\": {\"IN\": rec_positive_adjective}, \"OP\": \"+\"}]\n",
        "    matcher.add(\"P_IS_IT\",\n",
        "                on_match = on_match_p_is_it,\n",
        "                patterns = [is_it_pattern])\n",
        "\n",
        "\n",
        "def on_match_p_is_it(matcher, doc, id, matches):\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_DOES_ANYONE\n",
        "# Query about someone having specific knowledge\n",
        "# [Do|Does] we|anyone|someone <action_verbs> [...] <rec_target_noun>\n",
        "def setup_p_does_anyone(matcher):\n",
        "    does_pattern = [{\"LOWER\": {\"IN\": [\"does\", \"do\"]}, \"OP\": \"?\"},\n",
        "                    {\"LOWER\": {\"IN\": [\"we\", \"anyone\", \"someone\"]}},\n",
        "                    {\"LEMMA\": {\"IN\": action_verbs}},\n",
        "                    {\"LEMMA\": {\"NOT_IN\": rec_target_noun}, \"OP\": \"*\"},\n",
        "                    {\"LEMMA\": {\"IN\": rec_target_noun}, \"OP\": \"+\"}]\n",
        "    matcher.add(\"P_DOES_ANYONE\", on_match_p_does_anyone, does_pattern)\n",
        "\n",
        "def on_match_p_does_anyone(matcher, doc, id, matches):\n",
        "    #print('P_DOES_ANYONE Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_HAS_ANYONE\n",
        "# Query about someone having specific knowledge\n",
        "# Has|Have anyone|someone|you <action_verbs> <noun_phrase>\n",
        "def setup_p_has_anyone(matcher):\n",
        "    has_pattern = [{\"LOWER\": {\"IN\": [\"has\", \"have\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"you\", \"anyone\", \"someone\"]}},\n",
        "                    {\"LEMMA\": {\"IN\": action_verbs}}] + \\\n",
        "                    np_pattern\n",
        "    matcher.add(\"P_DOES_ANYONE\", on_match_p_has_anyone, has_pattern)\n",
        "\n",
        "def on_match_p_has_anyone(matcher, doc, id, matches):\n",
        "    #print('P_HAS_ANYONE Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_ANY\n",
        "# Question starting with any and followed by a rec target\n",
        "# Any <rec_target_noun> <noun_phrase>\n",
        "def setup_p_any(matcher):\n",
        "    base_pattern = [{\"LOWER\": \"any\"},\n",
        "                    {\"LOWER\": {\"IN\": rec_positive_adjective}, \"OP\": \"+\"}]\n",
        "    vp_narrow_pattern = [{\"POS\": {\"NOT_IN\": [\"VERB\"]}, \"OP\": \"*\"},\n",
        "                         {\"POS\": \"VERB\"}]\n",
        "    #not_i_pattern = [{\"TEXT\": {\"NOT_IN\": [\"I\"]}}]\n",
        "    any_pattern1 = base_pattern + [{\"LEMMA\": {\"IN\": rec_target_noun}}] + vp_narrow_pattern\n",
        "    any_pattern2 = base_pattern + [{\"TEXT\": {\"REGEX\": all_caps_noun_regex}}] + vp_narrow_pattern\n",
        "    any_pattern3 = [{\"LOWER\": \"any\"}, {\"LEMMA\": {\"IN\": [\"recommend\",\"recommendation\",\"advice\",\"suggestion\",\"alternative\",\"direction\"]}}]\n",
        "    matcher.add(\"P_ANY\", on_match=on_match_p_any, patterns=[any_pattern1, any_pattern2, any_pattern3])\n",
        "\n",
        "def on_match_p_any(matcher, doc, id, matches):\n",
        "    #print('P_ANY Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_WHERE_FIND\n",
        "# Question about the location of a rec item\n",
        "# Where can|could I|someone|anyone find <rec_target_noun>\n",
        "def setup_p_where(matcher):\n",
        "    end_pattern =  [{\"LOWER\": \"find\"},\n",
        "                    {\"LEMMA\": {\"NOT_IN\": rec_target_noun}, \"OP\": \"*\"},\n",
        "                    {\"LEMMA\": {\"IN\": rec_target_noun}, \"OP\": \"+\"}]\n",
        "    where_pattern1 = [{\"LOWER\": \"where\"},\n",
        "                    {\"LOWER\": {\"IN\": [\"can\",\"could\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"i\", \"anyone\", \"someone\"]}},\n",
        "                    {\"LOWER\": \"find\"}] + end_pattern\n",
        "    where_pattern2 = [{\"LOWER\": \"where\"},\n",
        "                    {\"LOWER\": \"i\"},\n",
        "                    {\"LOWER\": {\"IN\": [\"can\",\"could\"]}}] + end_pattern\n",
        "    matcher.add(\"P_ANY\", on_match=on_match_p_where, patterns=[where_pattern1, where_pattern2])\n",
        "\n",
        "def on_match_p_where(matcher, doc, id, matches):\n",
        "    #print('P_WHERE Matched!', doc.text)\n",
        "    pass\n",
        "\n",
        "\n",
        "# P_SHOULD_I_OR\n",
        "def setup_p_should_i_or(matcher):\n",
        "    or_pattern1 = [{\"LOWER\": {\"IN\": [\"should\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"i\"]}}] + \\\n",
        "                    np_pattern + \\\n",
        "                  [{\"LOWER\": {\"NOT_IN\": [\"or\"]}, \"OP\":\"*\"},\n",
        "                   {\"LOWER\": {\"IN\": [\"or\"]}}]\n",
        "    or_pattern2 = [{\"LOWER\": {\"IN\": [\"or\"]}},\n",
        "                   {\"LOWER\": {\"IN\": [\"should\"]}},\n",
        "                   {\"LOWER\": {\"IN\": [\"i\"]}}]\n",
        "\n",
        "    matcher.add(\"P_DOES_ANYONE\", on_match=on_match_p_should_i_or, patterns=[or_pattern1, or_pattern2])\n",
        "\n",
        "def on_match_p_should_i_or(matcher, doc, id, matches):\n",
        "    pass\n",
        "\n",
        "\n",
        "# belong to a single speaker\n",
        "def filter_potential_questions(all_uter):\n",
        "    mask = []\n",
        "    last_thread = -1\n",
        "    same_speaker = \"None\"\n",
        "    for index, row in all_uter.iterrows():\n",
        "        if row[\"thread\"] != last_thread:\n",
        "            last_thread = row[\"thread\"]\n",
        "            same_speaker = row[\"speaker\"]\n",
        "            mask.append(True)\n",
        "        elif row[\"speaker\"] == same_speaker:\n",
        "            mask.append(True)\n",
        "        else:\n",
        "            same_speaker = \"None\"\n",
        "            mask.append(False)\n",
        "    return all_uter[mask]\n",
        "\n",
        "\n",
        "def remove_punct_from_doc(doc):\n",
        "    indexes = []\n",
        "    for index, token in enumerate(doc):\n",
        "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
        "            indexes.append(index)\n",
        "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, LEMMA])\n",
        "    np_array = numpy.delete(np_array, indexes, axis = 0)\n",
        "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in indexes])\n",
        "    doc2.from_array([LOWER, POS, ENT_TYPE, LEMMA], np_array)\n",
        "    return doc2\n",
        "\n",
        "\n",
        "def print_err_diagnostics(data, sent_matches, index):\n",
        "    if (\"is_rec_question\" in data.columns):\n",
        "        if sent_matches:\n",
        "            if (data[\"is_rec_question\"][index] == 0):\n",
        "                print(\"False Positive,\\\"\" + str(data[\"thread\"][index]) + \": \" + data[\"message\"][index])\n",
        "                #print(\"False Positive,\\\"\" + data[\"message\"][index] + \"\\\"\")\n",
        "        else:\n",
        "            if (data[\"is_rec_question\"][index] == 1):\n",
        "                print(\"False Negative,\\\"\" + data[\"message\"][index] + \"\\\"\")\n",
        "\n",
        "\n",
        "def process_matches(q_data, matcher):\n",
        "    uter_rec_q = []\n",
        "    for index, row in q_data.iterrows():\n",
        "        uter = row['message']\n",
        "        thread = row['thread']\n",
        "\n",
        "        if (not isinstance(uter, str)) or '```' in uter:\n",
        "            uter_rec_q.append(0)\n",
        "            continue\n",
        "\n",
        "        sent_matches = False\n",
        "        matches = []\n",
        "        for sentence in nlp(uter).sents:\n",
        "            sent_nlp = nlp(sentence.text)\n",
        "            doc = remove_punct_from_doc(sent_nlp)\n",
        "            matches = matcher(doc)\n",
        "            if (len(matches) >= 1):\n",
        "                sent_matches = True\n",
        "        print_err_diagnostics(q_data, sent_matches, index)\n",
        "\n",
        "        if sent_matches:\n",
        "            uter_rec_q.append(thread)\n",
        "        else:\n",
        "            uter_rec_q.append(0)\n",
        "\n",
        "    return uter_rec_q\n",
        "\n",
        "\n",
        "def run_patterns_dev_set(matcher):\n",
        "    data = pd.read_csv('../../data/question/test_set.csv')\n",
        "    q_data = filter_potential_questions(data)\n",
        "\n",
        "    #print(\"Total convos:\", len(data[\"thread\"].unique()))\n",
        "    #print(\"Total rec questions:\", len(data.loc[data['is_rec_question'] == 1]))\n",
        "\n",
        "    uter_rec_q = process_matches(q_data, matcher)\n",
        "\n",
        "    assert(len(uter_rec_q) == len(q_data['is_rec_question']))\n",
        "    precision, recall, fscore, support = \\\n",
        "        precision_recall_fscore_support(q_data['is_rec_question'], [int(u != 0) for u in uter_rec_q], average='binary')\n",
        "\n",
        "    print('precision: {}'.format(precision))\n",
        "    print('recall: {}'.format(recall))\n",
        "    print('fscore: {}'.format(fscore))\n",
        "    print('support: {}'.format(support))\n",
        "\n",
        "\n",
        "def run_patterns_test_set_single(chatFileName, matcher):\n",
        "    data = pd.read_csv(chatFileName)\n",
        "\n",
        "    q_data = filter_potential_questions(data)\n",
        "    uter_rec_q = process_matches(q_data, matcher)\n",
        "    return data.loc[data['thread'].isin(uter_rec_q)]\n",
        "\n",
        "\n",
        "def run_patterns_test_set(matcher):\n",
        "    # generating train set with conversations with rec-asking questions\n",
        "    rec_threads = 0\n",
        "\n",
        "    outfile = 'outfile.csv'\n",
        "    if os.path.exists(outfile):\n",
        "        os.remove(outfile)\n",
        "\n",
        "    walkdir = \"../../data/irc\"\n",
        "    for root, dirnames, filenames in os.walk(walkdir):\n",
        "        for filename in filenames:\n",
        "            if not filename.endswith('.csv'):\n",
        "                continue\n",
        "            else:\n",
        "                chatFileName = os.path.join(root, filename)\n",
        "                print(chatFileName)\n",
        "\n",
        "                out_df = run_patterns_test_set_single(chatFileName, matcher)\n",
        "                ths = len(out_df.thread.unique())\n",
        "                print(\"Conversations with rec-asking questions:\", ths)\n",
        "                rec_threads = rec_threads + ths\n",
        "\n",
        "                #write a new csv file containing only the conversations which start with rec-asking question\n",
        "                with open(outfile, 'a') as fpointer:\n",
        "                    #local_path = [chatFileName] * len(df.index)\n",
        "                    #df['FileName'] = local_path\n",
        "                    out_df['thread'] = out_df['thread'].apply(lambda x: str(x) + '_' + str(root)[11:].replace(\"/\",\"_\"))\n",
        "                    assert ths == len(out_df.thread.unique())\n",
        "                    out_df.to_csv(fpointer, header=False, index=False)\n",
        "    print(\"Overall conversations with rec-asking questions:\", rec_threads)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    matcher = Matcher(nlp.vocab, validate=True)\n",
        "    setup_p_what_recadj_targetnoun(matcher)\n",
        "    setup_p_could_anyone(matcher)\n",
        "    setup_p_are_there(matcher)\n",
        "    setup_p_any(matcher)\n",
        "    setup_p_does_anyone(matcher)\n",
        "    setup_p_has_anyone(matcher)\n",
        "    setup_p_where(matcher)\n",
        "    setup_p_should_i_or(matcher)\n",
        "    setup_p_is_it(matcher)\n",
        "    #setup_p_how_do_i(matcher)\n",
        "\n",
        "    run_patterns_dev_set(matcher)\n",
        "    #run_patterns_test_set(matcher)\n",
        "\n",
        "##########################################former_data_helper.py##########################################################################\n",
        "if __name__ == '__main__':\n",
        "    df_train = pd.read_csv('../../data/answer/train_set.csv')\n",
        "    df_test = pd.read_csv('../../data/answer/test_set.csv')\n",
        "\n",
        "\n",
        "    def text_preprocess(message):\n",
        "        message = str(message)\n",
        "        message = re.sub('<http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url',\n",
        "                         message)  # Replace urls\n",
        "        message = re.sub('<@[A-Z][a-z]*>', ' username', message)  # Replace user mentions\n",
        "        message = re.sub(':[^:\\s]*(?:::[^:\\s]*)*:', 'emoji', message)  # Replace emoji\n",
        "        message = re.sub('’', '\\'', message)  # handle utf encodings\n",
        "        message = expand.expand_contractions(message)  # Expand contractions\n",
        "        message = re.sub('```([^`]*)```', 'code', message)  # Replace only multiline code\n",
        "        message = re.sub('`([^`]*)`', 'code', message)  # Replace any code\n",
        "        message = message.lower()  # Lower the text\n",
        "        message = message.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "        return message\n",
        "\n",
        "\n",
        "    def get_GloVe_embeddings():\n",
        "        # Load the customized GloVe embeddings trained on SO and put in Data Folder\n",
        "        embeddings_index = {}\n",
        "        f = open('../../data/WordEmbeddings/SO_vectors.txt', 'rb')\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "        return embeddings_index\n",
        "\n",
        "\n",
        "    def get_embedding_matrix(embeddings_index, word_index, num_words, embedding_dim, max_features):\n",
        "        # first create a matrix of zeros, this is our embedding matrix\n",
        "        embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "        # for each word in out tokenizer lets try to find that work in our w2v model\n",
        "        for word, i in word_index.items():\n",
        "            if i > max_features:\n",
        "                continue\n",
        "            embedding_vector = embeddings_index.get(bytes(word, 'utf-8'))\n",
        "            if embedding_vector is not None:\n",
        "                # add word vector to the matrix\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            else:\n",
        "                # if word doesn't exist, assign a random vector within our embedding range\n",
        "                embedding_matrix[i] = np.random.uniform(low=0.001, high=0.009, size=(embedding_dim,))\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "    def get_conv_len(df):\n",
        "        conv_lengths = []\n",
        "        grouped_by_conv = df.groupby(['thread'], sort=False)\n",
        "        for thread, conv in grouped_by_conv:\n",
        "            conv_lengths.append(conv.shape[0])\n",
        "        return conv_lengths\n",
        "\n",
        "\n",
        "    def create_utterance_embeddings(X, embedding_matrix):\n",
        "        X_repr = []\n",
        "        for utterance in X:\n",
        "            vec = []\n",
        "            for word in utterance:\n",
        "                try:\n",
        "                    vec.append(embedding_matrix[word])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "            X_repr.append(vec)\n",
        "        X_repr = np.asarray(X_repr)\n",
        "        return X_repr\n",
        "\n",
        "\n",
        "    def pad_nested_sequences(sequences, padding_len):\n",
        "        pad_seq = []\n",
        "        for seq in sequences:\n",
        "            x = []\n",
        "            count = 0\n",
        "            vec = np.zeros(200)\n",
        "            for word in seq:\n",
        "                if count < padding_len:\n",
        "                    x.append(word)\n",
        "                    count += 1\n",
        "            if len(x) < padding_len:\n",
        "                for i in range(padding_len - len(x)):\n",
        "                    x.append(vec)\n",
        "            pad_seq.append(x)\n",
        "        pad_seq = np.asarray(pad_seq)\n",
        "        return pad_seq\n",
        "\n",
        "\n",
        "    def create_sets(df, tokenizer, embedding_matrix):\n",
        "        # Data pre-processing\n",
        "        df['text'] = df['message'].apply(text_preprocess)\n",
        "        # this takes our sentences and replaces each word with an integer\n",
        "        X = tokenizer.texts_to_sequences(df['text'].values)\n",
        "        # Create embeddings\n",
        "        X0 = create_utterance_embeddings(X, embedding_matrix)\n",
        "        X1 = pad_nested_sequences(X0, 20)\n",
        "        Y1 = df['is_rec'].values\n",
        "        X2 = []\n",
        "        Y2 = []\n",
        "        count = 0\n",
        "        # get all conversation lengths\n",
        "        conv_lengths = get_conv_len(df)\n",
        "        sample_weights = []\n",
        "        # Creating joint QA representations\n",
        "        for conv in conv_lengths:\n",
        "            x_temp = []\n",
        "            y_temp = []\n",
        "            weights = []\n",
        "            for length in range(conv):\n",
        "                if length == 0:\n",
        "                    ques_repr = X1[count]\n",
        "                else:\n",
        "                    candidate_ans_repr = X1[count]\n",
        "                    concat_QA = np.concatenate((ques_repr, candidate_ans_repr), axis=0)\n",
        "                    x_temp.append(concat_QA)\n",
        "\n",
        "                    if Y1[count] == 2:\n",
        "                        y_temp.append(1)\n",
        "                        weights.append(1.0)\n",
        "                    else:\n",
        "                        y_temp.append(0)\n",
        "                        weights.append(1.0)\n",
        "                count += 1\n",
        "            X2.append(x_temp)\n",
        "            Y2.append(y_temp)\n",
        "            sample_weights.append(weights)\n",
        "\n",
        "        X2 = np.asarray(X2)\n",
        "        # Padding data for LSTM\n",
        "        X2 = pad_sequences(X2, maxlen=20, padding=\"post\", truncating=\"post\", dtype='float32', value=0.)\n",
        "        Y2 = pad_sequences(Y2, maxlen=20, padding=\"post\", truncating=\"post\")\n",
        "        sample_weights = pad_sequences(sample_weights, maxlen=20, padding=\"post\", truncating=\"post\", value=0.0)\n",
        "        Y2 = np.asarray(Y2)\n",
        "        Y2 = Y2.reshape(Y2.shape[0], Y2.shape[1], 1)\n",
        "        return X2, Y2, sample_weights\n",
        "\n",
        "\n",
        "    def prep_data(df_train, df_test, embedding_dim):\n",
        "        df = pd.concat([df_train, df_test])\n",
        "        # Data pre-processing\n",
        "        df['text'] = df['message'].apply(text_preprocess)\n",
        "        # Build vocabulary\n",
        "        max_features = 30000  # number of words we care about\n",
        "        tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
        "        tokenizer.fit_on_texts(df['text'].values)\n",
        "        word_index = tokenizer.word_index\n",
        "        print('Found %s unique tokens in vocab' % len(word_index))\n",
        "        num_words = min(max_features, len(word_index)) + 1\n",
        "        # get all GloVe embeddings\n",
        "        embeddings_index = get_GloVe_embeddings()\n",
        "        # get embedding matrix for words in our vocab\n",
        "        embedding_matrix = get_embedding_matrix(embeddings_index, word_index, num_words, embedding_dim, max_features)\n",
        "        X_train, Y_train, sample_weights_train = create_sets(df_train, tokenizer, embedding_matrix)\n",
        "        X_test, Y_test, sample_weights_test = create_sets(df_test, tokenizer, embedding_matrix)\n",
        "        return X_train, Y_train, X_test, Y_test, sample_weights_train, sample_weights_test\n",
        "    ####################################################################################################################\n",
        "\n",
        "\n",
        "    embedding_dim = 200  # dim of our learned embedding model\n",
        "    X_train, Y_train, X_test, Y_test, sample_weights_train, sample_weights_test = prep_data(df_train, df_test, embedding_dim)\n",
        "    utterance_length = 40\n",
        "    conversation_length = 20\n",
        "    inputs = Input(shape=((conversation_length), utterance_length, embedding_dim,))\n",
        "\n",
        "    # CNN Layer\n",
        "    num_filters = 50\n",
        "    reshape = Reshape((conversation_length, utterance_length, embedding_dim, 1))(inputs)\n",
        "    conv_0_3 = TimeDistributed(Conv2D(num_filters, kernel_size=(2, embedding_dim), activation='relu'), input_shape=(1, conversation_length, utterance_length, embedding_dim, 1))(reshape)\n",
        "    maxpool_0_3 = TimeDistributed(MaxPool2D(pool_size=(2, 1), padding='valid'))(conv_0_3)\n",
        "    conv_1_3 = TimeDistributed(Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu'), input_shape=(1, conversation_length, utterance_length, embedding_dim, 1))(reshape)\n",
        "    maxpool_1_3 = TimeDistributed(MaxPool2D(pool_size=(2, 1), padding='valid'))(conv_1_3)\n",
        "    conv_2_3 = TimeDistributed(Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu'), input_shape=(1, conversation_length, utterance_length, embedding_dim, 1))(reshape)\n",
        "    maxpool_2_3 = TimeDistributed(MaxPool2D(pool_size=(3, 1), padding='valid'))(conv_2_3)\n",
        "    concatenated_tensor = Concatenate(axis=2)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
        "    flatten = TimeDistributed(Flatten())(concatenated_tensor)\n",
        "    output = Dropout(0.5)(flatten)\n",
        "\n",
        "    # biLSTM Layer\n",
        "    bilstm = Bidirectional(LSTM(units=200, return_sequences=True, recurrent_dropout=0.1))(output)  # variational biLSTM)\n",
        "    outputs = TimeDistributed(Dense(1, activation=\"sigmoid\"))(bilstm)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    opt = adam(lr=0.001)\n",
        "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"], sample_weight_mode='temporal')\n",
        "    print(model.summary())\n",
        "\n",
        "    ###################################################former_evaluation.py#############################################################\n",
        "    import pandas as pd\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    import time\n",
        "    from matplotlib import pyplot\n",
        "    from numpy.random import seed\n",
        "    from tensorflow import set_random_seed\n",
        "\n",
        "    seed(1)\n",
        "    set_random_seed(2)\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    def write_csv(test_labels, pred_labels):\n",
        "        for list in test_labels:\n",
        "            list.insert(0, 'Question')\n",
        "        for list in pred_labels:\n",
        "            list.insert(0, 'Question')\n",
        "        test_file = \"../../data/answer/test_set.csv\"\n",
        "        test_df = pd.read_csv(test_file)\n",
        "        grouped_by_conv = test_df.groupby(['thread'], sort=False)\n",
        "        count = 0\n",
        "\n",
        "        for thread, conv in grouped_by_conv:\n",
        "            if conv.shape[0] != len(test_labels[count]):\n",
        "                for i in range(conv.shape[0] - len(test_labels[count])):\n",
        "                    test_labels[count].append(0)\n",
        "                    pred_labels[count].append(0)\n",
        "            count += 1\n",
        "\n",
        "        test_list = [item for sublist in test_labels for item in sublist]\n",
        "        pred_list = [item for sublist in pred_labels for item in sublist]\n",
        "        test_df['TrueLabels'] = test_list\n",
        "        test_df['PredLabels'] = pred_list\n",
        "        test_df.to_csv(\"test_ChatE_pred.csv\", index=False)\n",
        "\n",
        "\n",
        "    def calc_eval_measures():\n",
        "        print(\"Evaluation Measures:\")\n",
        "        df = pd.read_csv('./test_ChatE_pred.csv')\n",
        "        true = df['TrueLabels'].values.tolist()\n",
        "        true = [x for x in true if x != 'Question']  # discarding question instances\n",
        "        true = ['1' if x == 'Answer' else x for x in true]  # converting to binary values\n",
        "        true = ['0' if x == 'NonAnswer' else x for x in true]  # converting to binary values\n",
        "\n",
        "        pred = df['PredLabels'].values.tolist()\n",
        "        pred = [x for x in pred if x != 'Question']  # discarding question instances\n",
        "        pred = ['1' if x == 'Answer' else x for x in pred]  # converting to binary values\n",
        "        pred = ['0' if x == 'NonAnswer' else x for x in pred]  # converting to binary values\n",
        "\n",
        "        precision, recall, fscore, support = precision_recall_fscore_support(true, pred, average='binary',\n",
        "                                                                             pos_label='1')\n",
        "        print('precision: {:.2f}'.format(precision))\n",
        "        print('recall: {:.2f}'.format(recall))\n",
        "        print('fscore: {:.2f}'.format(fscore))\n",
        "\n",
        "\n",
        "    def pred2binarylabel(pred, sample_weights):\n",
        "        idx2tag = {0: 'NonAnswer', 1: 'Answer'}\n",
        "        out = []\n",
        "        for i, pred_i in enumerate(pred):\n",
        "            out_i = []\n",
        "            for j, p in enumerate(pred_i):\n",
        "                if p >= 0.5 and (sample_weights[i][j] == 1.0):\n",
        "                    out_i.append(idx2tag[1])\n",
        "                elif p < 0.5 and (sample_weights[i][j] == 1.0):\n",
        "                    out_i.append(idx2tag[0])\n",
        "            out.append(out_i)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def train_test_validate(model, X_tr, Y_tr, X_te, Y_te, sample_weights_train, sample_weights_test):\n",
        "        # Fit the model on train data\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "        history = model.fit(X_tr, Y_tr, batch_size=256, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
        "        pyplot.plot(history.history['loss'], label='train_loss')\n",
        "        pyplot.plot(history.history['val_loss'], label='val_loss')\n",
        "        pyplot.legend()\n",
        "        pyplot.show()\n",
        "\n",
        "        # Evaluate the model on test data\n",
        "        scores = model.evaluate(X_te, Y_te, verbose=0)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "        test_pred = model.predict(X_te, verbose=1)\n",
        "        pred_labels = pred2binarylabel(test_pred, sample_weights_test)\n",
        "        test_labels = pred2binarylabel(Y_te, sample_weights_test)\n",
        "        print(\"Model took\", time.time() - start_time, \"to run\")\n",
        "        write_csv(test_labels, pred_labels)  # write predicted outputs\n",
        "        calc_eval_measures()  # calculate evaluation measures\n",
        "    ####################################################################################################################\n",
        "    # Evaluation\n",
        "    train_test_validate(model, X_train, Y_train, X_test, Y_test, sample_weights_train, sample_weights_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Found 18273 unique tokens in vocab\n",
        "Found 60000 word vectors.\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "...\n",
        "Total params: 3,245,601\n",
        "Trainable params: 3,245,601\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________"
      ],
      "metadata": {
        "id": "twKWlUX2x6Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluation Measures:\n",
        "precision: 0.535\n",
        "recall: 0.510\n",
        "fscore: 0.500\n",
        "\n",
        "Evaluation Measures:\n",
        "precision: 0.462\n",
        "recall: 0.421\n",
        "fscore: 0.414\n"
      ],
      "metadata": {
        "id": "JZKKg51fwlkd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}